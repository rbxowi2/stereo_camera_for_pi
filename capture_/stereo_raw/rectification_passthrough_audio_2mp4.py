import cv2
import numpy as np
import os
import glob
import sys
from datetime import datetime
from pydub import AudioSegment
import subprocess
import re

# ---------- è¨­å®š ----------
raw_pattern = "frame_*.raw"                # åŸå§‹ raw æª”è·¯å¾‘æ ¼å¼ï¼ˆå·¦å³æ°´å¹³ä¸¦åˆ—ï¼‰

stereo_param_file = "stereo_camera_params.npz"

txt_files = glob.glob("recording_*.txt")
txt_file = txt_files[0]

audio_file =glob.glob("recording_*.wav")
audio_file = audio_file[0]


output_video_path = "sph_rectified_eye_relief_adapted.mp4"
video_codec = "mp4v"
frame_rate = 12

# è¼¸å‡º equirect å¤§å°ï¼ˆæ°´å¹³å°æ‡‰ 180Â°ï¼‰
w_out_ = 4000   # å»ºè­°æ ¹æ“šæ•ˆèƒ½èª¿æ•´ï¼ˆè¶Šå¤§è¶Šæ…¢ï¼‰
h_out_ = w_out_

# Eye reliefï¼ˆå–®ä½ mmï¼‰
target_eye_relief = 15.0

# æ˜¯å¦å° remapped çµæœåš inpaintï¼ˆæ´è£œï¼‰
hole_inpaint = False
inpaint_radius = 3

# StereoSGBM åƒæ•¸ï¼ˆå¯ä¾æƒ…æ³å¾®èª¿ï¼‰
minDisparity = 1
numDisparities = 64  # å¿…é ˆæ˜¯ 16 çš„å€æ•¸
blockSize = 1

# ====== è®€å– txt æª”ï¼Œè§£æ Audio_Start Time èˆ‡æ¯å¹€æ™‚é–“æˆ³ ======
def parse_txt_timestamps(txt_path):
    with open(txt_path, "r") as f:
        lines = f.readlines()

    audio_start_line = lines[0]
    audio_start_time_str = re.search(r'Audio_Start Time: (.+)', audio_start_line).group(1)
    audio_start_time = datetime.fromisoformat(audio_start_time_str)

    # å¹€æ™‚é–“æˆ³åˆ—è¡¨
    frame_times = []
    for line in lines[3:]:  # å¾ç¬¬å››è¡Œé–‹å§‹ç‚ºå¹€æ™‚é–“æˆ³
        parts = line.strip().split()
        if len(parts) == 2:
            # æ ¼å¼: "00000 2025-08-04T16:04:11.623"
            frame_times.append(datetime.fromisoformat(parts[1]))
    return audio_start_time, frame_times

audio_start_time, frame_times = parse_txt_timestamps(txt_file)
total_frames = len(frame_times)
if total_frames == 0:
    print("âŒ æ²’æœ‰è®€å–åˆ°ä»»ä½•æ™‚é–“æˆ³")
    sys.exit(1)

# ---------- æª¢æŸ¥æª”æ¡ˆ ----------
raw_files = sorted(glob.glob(raw_pattern))
if len(raw_files) == 0:
    print("âŒ æœªæ‰¾åˆ°ä»»ä½• RAW æª”æ¡ˆï¼Œè«‹ç¢ºèªæª”åæ ¼å¼")
    sys.exit(1)

if not os.path.exists(stereo_param_file):
    print(f"âŒ æ‰¾ä¸åˆ°åƒæ•¸æª”: {stereo_param_file}")
    sys.exit(1)

data = np.load(stereo_param_file)
K_l = data["K_l"]
D_l = data["D_l"]
K_r = data["K_r"]
D_r = data["D_r"]
R = data["R"]
T = data["T"]
DIM = tuple(map(int, data["DIM"]))

# ---------- stereo rectifyï¼ˆfisheyeï¼‰ ----------
# ä¸­ä»‹è§£æåº¦ (rectified ç”¨)
DIM2 = (int(w_out_ * 1.2), int(w_out_ * 1.2))  # å¯ä»¥èª¿æ•´ speed/accuracy tradeoff

R1, R2, P1, P2, Q = cv2.fisheye.stereoRectify(
    K_l, D_l, K_r, D_r, DIM, R, T,
    flags=cv2.CALIB_ZERO_DISPARITY,
    newImageSize=DIM2,
    balance=0.0, fov_scale=1.0
)

# å°‡ principal point move åˆ°ä¸­å¿ƒï¼ˆè¦–éœ€è¦ï¼‰
P1[0,2] = DIM2[0] / 2.0
P1[1,2] = DIM2[1] / 2.0
P2[0,2] = DIM2[0] / 2.0
P2[1,2] = DIM2[1] / 2.0

Q[0, 3] = -DIM2[0]/2
Q[1, 3] = -DIM2[1]/2

map1_l, map2_l = cv2.fisheye.initUndistortRectifyMap(K_l, D_l, R1, P1, DIM2, cv2.CV_16SC2)
map1_r, map2_r = cv2.fisheye.initUndistortRectifyMap(K_r, D_r, R2, P2, DIM2, cv2.CV_16SC2)

# ---------- Stereo Matcher ----------
stereo = cv2.StereoSGBM_create(
    minDisparity=minDisparity,
    numDisparities=numDisparities,
    blockSize=blockSize,
    P1=8*3*blockSize**2,
    P2=32*3*blockSize**2,
    mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY
)

# ---------- å»ºç«‹ equirect çš„ rays èˆ‡åŸºæœ¬æŠ•å½±çŸ©é™£ ----------
# K_ ç”¨æ–¼æŠŠæ–¹å‘å‘é‡ (ray) æŠ•å½±åˆ° rectified ä¸­ä»‹ç›¸æ©Ÿå¹³é¢ (camera intrinsics)
# ç”¨ P1 çš„ç„¦è·èˆ‡ä¸­å¿ƒä¾†å»ºç«‹
K_center = np.array([
    [P1[0,0], 0, DIM2[0] / 2.0],
    [0, P1[1,1], DIM2[1] / 2.0],
    [0, 0, 1.0]
], dtype=np.float32)

w_out = w_out_
h_out = h_out_

# lon: -90Â°..+90Â° (VR180 æ°´å¹³)
lon = np.linspace(-np.pi/2, np.pi/2, w_out)
#lat = np.linspace(np.pi/2, -np.pi/2, h_out)
lat = np.linspace(-np.pi / 2, np.pi / 2, h_out) 
lon_map, lat_map = np.meshgrid(lon, lat)

# çƒé¢æ–¹å‘å‘é‡ (camera coord)
x = np.cos(lat_map) * np.sin(lon_map)
y = np.sin(lat_map)
z = np.cos(lat_map) * np.cos(lon_map)
rays = np.stack([x, y, z], axis=-1).astype(np.float32)  # shape (h_out, w_out, 3)

# æŠ•å½± rays åˆ° image plane (ä¸å«æ·±åº¦)
proj = rays @ K_center.T
z_safe_proj = np.where(proj[..., 2] == 0, 1e-6, proj[..., 2])
u = proj[..., 0] / z_safe_proj
v = proj[..., 1] / z_safe_proj

# å°æ–¼ remap çš„åŸºæœ¬ valid maskï¼ˆåœ¨ rectified åœ–ç¯„åœå…§ï¼‰
h_in, w_in = DIM2[1], DIM2[0]
valid_basic = (u >= 0) & (u < w_in) & (v >= 0) & (v < h_in)

# æŠŠ map_x/map_y å…ˆå­˜èµ·ä¾†ï¼ˆç”¨æ–¼æŠŠæ¯å€‹ equirect pixel sample depthï¼‰
map_x_basic = np.where(valid_basic, u, -1).astype(np.float32)
map_y_basic = np.where(valid_basic, v, -1).astype(np.float32)

# ====== è™•ç†éŸ³è¨Šï¼Œå‰ªè£èˆ‡å½±ç‰‡æ™‚é–“å°é½Š ======
audio = AudioSegment.from_wav(audio_file)
video_start = frame_times[0]
delta_ms = int((video_start - audio_start_time).total_seconds() * 1000)

if delta_ms > 0:
    trimmed_audio = audio[delta_ms:]
else:
    trimmed_audio = AudioSegment.silent(duration=abs(delta_ms)) + audio

video_duration_ms = int((frame_times[-1]-frame_times[0]).total_seconds() * 1000)
trimmed_audio = trimmed_audio[:video_duration_ms]
trimmed_audio.export("temp_audio.wav", format="wav")

# ---------- IO / é¡¯ç¤ºæº–å‚™ ----------
cv2.namedWindow("combined_f", cv2.WINDOW_NORMAL)
video_writer = None

frame_interval = 1 / frame_rate
prev_time = None
prev_frame = None
init_map_ = False

# ---------- ä¸»è¿´åœˆ ----------
for idx, current_time in enumerate(frame_times):
    raw_file = f"frame_{idx:04d}.raw"
    # åˆ¤æ–·æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if os.path.exists(raw_file):
        # è®€ rawï¼ˆå‡è¨­ uint8 ä¸‰é€šé“ï¼Œå·¦å³æ°´å¹³ä¸¦åˆ—æ–¹å½¢ï¼‰
        with open(raw_file, "rb") as f:
            raw_data = f.read()
        frame_np = np.frombuffer(raw_data, dtype=np.uint8).copy().reshape((DIM[1], DIM[0]*2, 3))
        frame_l_raw = frame_np[:, :DIM[0], :]
        frame_r_raw = frame_np[:, DIM[0]:, :]
        
        # rectification -> rectified images (DIM2)
        rect_l = cv2.remap(frame_l_raw, map1_l, map2_l, interpolation=cv2.INTER_LINEAR)
        rect_r = cv2.remap(frame_r_raw, map1_r, map2_r, interpolation=cv2.INTER_LINEAR)
        
        # disparity (å° rectified åœ–)
        gray_l = cv2.cvtColor(rect_l, cv2.COLOR_BGR2GRAY)
        gray_r = cv2.cvtColor(rect_r, cv2.COLOR_BGR2GRAY)
        disp = stereo.compute(gray_l, gray_r).astype(np.float32) / 16.0  # disparity in pixels
        
        # optional: ä»¥ bilateral æˆ– median å»é›œè¨Šï¼ˆå¯é¸ï¼‰
        disp_filtered = cv2.medianBlur(disp, 5)
        
        # reproject to 3D 
        points_3d = cv2.reprojectImageTo3D(disp_filtered, Q)  # shape (h_in, w_in, 3)
        # å‹¿å°‡æ•´å€‹ points_3d è¤‡è£½åˆ°é«˜è§£æåº¦ï¼ˆæœƒçˆ†è¨˜æ†¶é«”ï¼‰ï¼Œç”¨ map_x_basic/map_y_basic ç›´æ¥ sample depth (Z)
        depth_map = points_3d[..., 2]  # Z (å–®ä½èˆ‡ T ç›¸åŒ)
        
        # sample depth åˆ° equirectï¼ˆä½¿ç”¨ map_x_basic/map_y_basicï¼‰
        depth_equi = cv2.remap(depth_map, map_x_basic, map_y_basic, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=-1.0)
        
        # invalid depth => nan
        depth_equi = np.where(depth_equi <= 0, np.nan, depth_equi).astype(np.float32)
        
        # å»ºç«‹æ¯åƒç´  3D é» = ray * depth
        # rays shape (h_out, w_out, 3), depth_equi (h_out, w_out)
        depth_safe = depth_equi[..., None]  # (h_out, w_out, 1)
        points3d_equi = rays * depth_safe  # (h_out, w_out, 3) in left rectified cam coords
        
        # Eye Relief è£œå„Ÿ
        points3d_new = points3d_equi.copy()
        points3d_new[..., 2] = points3d_new[..., 2] - target_eye_relief
        
        # æŠ•å½±å‡½æ•¸ï¼ˆå‘é‡åŒ–ï¼‰ï¼šæŠŠ 3D é»æŠ•å½±å› rectified ç›¸æ©Ÿå¹³é¢ï¼Œå† remap å–æ¨£ rectified åœ–
        def project_and_sample(points3d, K_mat, source_img):
            # points3d: (h_out, w_out, 3)
            proj = points3d @ K_mat.T  # (h_out,w_out,3)
            z_p = proj[..., 2]
            z_p_safe = np.where(np.abs(z_p) < 1e-6, 1e-6, z_p)
            u_p = proj[..., 0] / z_p_safe
            v_p = proj[..., 1] / z_p_safe
            
            # out-of-range æˆ– nan => -1
            invalid = np.isnan(u_p) | np.isnan(v_p) | (u_p < 0) | (u_p >= source_img.shape[1]) | (v_p < 0) | (v_p >= source_img.shape[0]) | (z_p <= 0)
            u_p = np.where(invalid, -1.0, u_p).astype(np.float32)
            v_p = np.where(invalid, -1.0, v_p).astype(np.float32)
            
            sampled = cv2.remap(source_img, u_p, v_p, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=(0,0,0))
            # mask of invalid pixels
            mask_invalid = invalid.astype(np.uint8)*255
            return sampled, mask_invalid
        
        # å› ç‚ºè¦ç”¢ç”Ÿå·¦å³çœ¼çš„ equirect ç•«é¢ï¼ˆå…©å¼µï¼‰ï¼Œå°å·¦çœ¼æ¡æ¨£ rect_lï¼›å°å³çœ¼æ¡æ¨£ rect_r
        # è‹¥è¦å·¦å³çœ¼åœ¨ Eye Relief æ–¹å‘å†åšå¾®èª¿ï¼ˆä¾‹å¦‚å·¦å³åç§»IPDï¼‰ï¼Œå¯ä»¥åœ¨æ­¤é¡å¤–å¹³ç§» Xã€‚
        res_equi_l, mask_l = project_and_sample(points3d_new, K_center, rect_l)
        res_equi_r, mask_r = project_and_sample(points3d_new, K_center, rect_r)
        
        # åˆä½µï¼ˆå·¦å³ä¸¦åˆ—ï¼‰
        combined_s = np.hstack((res_equi_l, res_equi_r))
        
        # ç°¡å–®çš„æ´å¡«è£œï¼šå…ˆç”¨ median å»é›œè¨Šï¼Œå†ä»¥ inpaint å¡«æ´ï¼ˆè‹¥å•Ÿç”¨ï¼‰
        if hole_inpaint:
            # åˆä½µå·¦å³ mask å†å¡«è£œå„åŠå¼µ
            mask_combined = np.hstack((mask_l, mask_r))
            # å…ˆç”¨ median å»æ‰å­¤ç«‹å°é»
            combined_s_m = cv2.medianBlur(combined_s, 5)
            # inpaint éœ€è¦å–®é€šé“ mask (0/255)
            inpainted = cv2.inpaint(combined_s_m, mask_combined, inpaint_radius, cv2.INPAINT_TELEA)
            display_img = inpainted
        else:
            display_img = combined_s
        
        prev_frame = display_img.copy()
    else:
        if prev_frame is not None:
            print(f"âš ï¸ {raw_file} ä¸å­˜åœ¨ï¼Œä½¿ç”¨ä¸Šä¸€å¹€è£œå¹€")
            display_img = prev_frame.copy()
        else:
            print(f"âŒ ç„¡æ³•è®€å–å¹€ä¸”ç„¡å‰ä¸€å¹€å¯è£œï¼Œè·³éç¬¬ {idx} å¹€")
            continue
    
    if video_writer is None:
        fourcc = cv2.VideoWriter_fourcc(*video_codec)
        video_writer = cv2.VideoWriter("temp_video.mp4", fourcc, frame_rate, (display_img.shape[1], display_img.shape[0]))
        init_VideoWriter = True
    
    # åˆ¤æ–·æ‰å¹€æ•¸
    if prev_time is not None:
        delta_t = (current_time - prev_time).total_seconds()
        num_missing = int(delta_t / frame_interval) - 1
        if num_missing > 0:
            for _ in range(num_missing):
                video_writer.write(prev_frame)
            print(f"ğŸ” æ‰å¹€è£œå¹€ {num_missing} å¹€: {prev_time} -> {current_time}")
    
    video_writer.write(display_img)
    prev_time = current_time
    
    cv2.imshow("combined_f", display_img)
    
    # é€²åº¦
    print(f"ğŸ“Š è™•ç†ä¸­: [{idx + 1}/{total_frames}] {((idx + 1) / total_frames) * 100:.2f}%", end='\r')
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# ---------- çµæŸ ----------
if video_writer is not None:
    video_writer.release()
cv2.destroyAllWindows()
print("\nâœ… å½±ç‰‡è™•ç†å®Œæˆï¼Œé–‹å§‹åˆä½µéŸ³è¨Š")

# ====== ffmpeg åˆä½µéŸ³è¨Šèˆ‡å½±ç‰‡ ======
cmd = [
    "ffmpeg", "-y",
    "-i", "temp_video.mp4",
    "-i", "temp_audio.wav",
    "-c:v", "copy",
    "-af", "volume=10.0",  # éŸ³é‡æ”¾å¤§ 10 å€ï¼ˆå¯èª¿æ•´ï¼‰
    "-c:a", "aac",
    "-strict", "experimental",
    output_video_path
]

subprocess.run(cmd)
print(f"âœ… åˆä½µå®Œæˆï¼Œè¼¸å‡ºæª”æ¡ˆ: {output_video_path}")

